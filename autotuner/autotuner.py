"""
Module: autotuner

Contains the function tune_kernels, which profiles the matrix multiplication 
kernels and returns the optimal parameters. Users would typically construct 
a MatMulKernel using the parameters for future computations.

The module also serves as a Python script which can be executed on the command 
line. In this mode, the main function will be invoked which is essentially a 
wrapper around tune_kernels using parameters specified by the command line 
arguments. After the optimal parameters are found, a kernel using these 
parameters are constructed and its source code written to the output file. A 
sample session using this interface is as follows

    $ python autotuner.py 1000 double -d 1

This launches an autotuner for 1000*1000 double precision matrices on CUDA 
device 1 and writes the tuned kernel source code to the file matmul.cu. To 
see the full documentation of the command line interface, call

    $ python autotuner.py -h

"""

import pycuda.driver as cuda
import numpy as np
from matmul import MatMulKernel

def tune_kernels(n, dtype, num_trials=5):
    """
    Profile matrix multiplication kernels with different parameters for
    matrices of size n*n and datatype dtype (numpy.float32 or numpy.float64).

    The run time is measured as an average of num_trials runs, and the 
    parameters for the best performing kernel is returned.
    """
    device = cuda.Context.get_device()
    print("Tuning matrix multiplication kernel for CUDA device " + 
        device.name())
    print("Matrix dimension: {0} x {0}".format(n))
    if dtype == np.float32:
        print("Precision: single")
    else:
        print("Precision: double")
    print("--------------------")

    # Construct random matrices as test input
    M = np.random.randn(n, n).astype(dtype)
    N = np.random.randn(n, n).astype(dtype)
    profile_result = {}
    
    # Determine the candidate tile widths
    # For now, a CUDA device with warp size of 32 is assumed. This means 
    # tile width should be a multiple of 8, and the maximum tile width is 
    # determined by the maximum threads per block property.
    max_multiple = int(np.sqrt(device.max_threads_per_block) / 8.0)
    TWs = [8 * k for k in range(1, max_multiple+1)]

    # Profile the kernels
    for tw in TWs:
        k1 = MatMulKernel(dtype, tw, loop_unroll=True)
        k2 = MatMulKernel(dtype, tw, loop_unroll=False)
        t1 = np.zeros(num_trials)
        t2 = np.zeros(num_trials)
        for i in range(num_trials):
            _, t1[i] = k1.matmul(M, N, timed=True)
            _, t2[i] = k2.matmul(M, N, timed=True)
        profile_result[(tw, True)] = np.average(t1)
        profile_result[(tw, False)] = np.average(t2)
        print("tile width = {}, unrolled loops: average run time = {} ms"
            .format(tw, profile_result[(tw, True)]))
        print("tile width = {}, full loops: average run time = {} ms"
            .format(tw, profile_result[(tw, False)]))

    # Choose the best result
    best = sorted(profile_result.items(), key = lambda x: x[1])[0]
    tile_width, loop_unroll = best[0]
    print("--------------------")
    print("Best kernel: tile width = {}, loop unroll = {}".format(tile_width, loop_unroll))
    return tile_width, loop_unroll

def main():
    # Parse command-line arguments
    import argparse
    parser = argparse.ArgumentParser(description=
        'An autotuner that generates the best CUDA matrix multiplication kernel.')
    parser.add_argument('n', type=int, help="matrix width")
    parser.add_argument('prec', nargs='?', default='double', 
        choices=['single', 'double'], help="precision (single or double, default to double)")
    parser.add_argument('-t', '--num-trials', type=int, default=5, 
        help="number of trials (default: %(default)s)")
    parser.add_argument('-d', '--CUDA-device', type=int, default=0,
        help="which CUDA device to tune on (default: %(default)s)")
    parser.add_argument('-o', '--output-file', default='matmul.cu',
        help="name of the output file (default: %(default)s)")
    args = parser.parse_args()
    if args.n <= 0:
        raise ValueError("Matrix width must be positive.")
    if args.prec == 'single':
        dtype = np.float32
    else:
        dtype = np.float64
    if args.num_trials <= 0:
        raise ValueError("Number of trials must be positive.")

    # Initialize CUDA runtime
    cuda.init()
    device = cuda.Device(args.CUDA_device)
    context = device.make_context()

    # Tune kernels
    tile_width, loop_unroll = tune_kernels(n=args.n, dtype=dtype, num_trials=args.num_trials)
    kernel = MatMulKernel(dtype, tile_width, loop_unroll, compile=False)
    with open(args.output_file, 'w') as f:
        f.write("/* Generated by autotuner.py */\n")
        f.write("/* tile width = {}, loop unroll = {} */\n"
            .format(tile_width, loop_unroll))
        f.write(kernel.src)
    context.pop()

if __name__ == '__main__':
    main()