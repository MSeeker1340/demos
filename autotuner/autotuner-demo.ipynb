{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform == Windows-10-10.0.16299 AMD64\n",
      "Python version == 2.7.14\n",
      "NumPy version == 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "# from pycuda import autoinit # set up the PyCUDA runtime\n",
    "\n",
    "print(\"Platform == \" + platform.platform() + ' ' + platform.machine())\n",
    "print(\"Python version == \" + platform.python_version())\n",
    "print(\"NumPy version == \" + np.__version__)\n",
    "# print(\"PyCUDA version == \" + pycuda.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Autotuning\n",
    "\n",
    "Autotuning is the idea of letting programs find the best parameters to an algorithm and generate the algorithm according to these parameters. It is a widely used technique in numerical computation software such as [ATLAS](http://math-atlas.sourceforge.net/) and [FFTW](http://www.fftw.org/). \n",
    "\n",
    "The use of autotuning for GPU kernels has become a hot research topic in the recent years. Two major approaches to autotuning are model-based tuning (which creates a computation model *a priori* based on the algorithm and architecture) and empirical tuning (which measures the performance as if the kernel is a black box). The latter is used for the `autotuner` package for the specific problem of GPU dense matrix multiplication.\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "Dense matrix multiplication is one of the most studied GPU applications. Although highly tuned implementations can be found with relative ease, an autotuner for matrix multiplication still has its benefits. This is because hand-tuned implementations for one device may not run as well on another device, while autotuners can be ported easily to new architectures with good performance. In addition, matrix multiplication kernels are relatively simple and serves as a good introduction to autotuner design.\n",
    "\n",
    "## PyCUDA and GPU metaprogramming\n",
    "\n",
    "[PyCUDA](https://mathema.tician.de/software/pycuda/) is a Python library for accessing Nvidiaâ€™s CUDA parallel computation API. One of the main strengths of PyCUDA is its ability to perform **just-in-time compilation (JIT)** of CUDA C source code. A programmer can create some kernels, compile and run them on the GPU, and then modify the existing kernels dynamically based on the results without leaving the Python interpreter. This process, when done in a program-controlled manner, enables **GPU metaprogramming**, which is crucial to the development of autotuners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `autotuner.py`\n",
    "\n",
    "When invoked as a Python script on the command line, `autotuner.py` parses the arguments and initiates a profiling session to find the optimal parameters for the matmul kernel (this is the autotuning part). After profiling is done, the source code for the optimal kernel is written out as a `.cu` file which can later be used either with PyCUDA or just as input to `nvcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run autotuner.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample session using `autotuner.py` looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run autotuner.py 2000 single -o kernel.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('kernel.cu', 'r') as f:\n",
    "#     print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A closer look at the internals\n",
    "\n",
    "The base building block of each kernel is the template file `template.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void matmul({real} *M, {real} *N, {real} *P, int Width)\n",
      "{{\n",
      "    // Compute M * N and store result in P\n",
      "    // M and N are Width * Width matrices\n",
      "    __shared__ {real} Ms[{TW}][{TW}];\n",
      "    __shared__ {real} Ns[{TW}][{TW}];\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "    int Row = blockIdx.y * {TW} + ty;\n",
      "    int Col = blockIdx.x * {TW} + tx;\n",
      "\n",
      "    {real} Pvalue = {fzero};\n",
      "    for (int ph = 0; ph < ceil(Width / ({real}){TW}); ++ph)\n",
      "    {{\n",
      "        // Cooperatively load tile into shared memory\n",
      "        if (Row < Width && ph*{TW} + tx < Width)\n",
      "        {{\n",
      "            Ms[ty][tx] = M[Row*Width + ph*{TW} + tx];\n",
      "        }}\n",
      "        else\n",
      "        {{\n",
      "            Ms[ty][tx] = {fzero};\n",
      "        }}\n",
      "        if (Col < Width && ph*{TW} + ty < Width)\n",
      "        {{\n",
      "            Ns[ty][tx] = N[(ph*{TW} + ty)*Width + Col];\n",
      "        }}\n",
      "        else\n",
      "        {{\n",
      "            Ns[ty][tx] = {fzero};\n",
      "        }}\n",
      "        __syncthreads();\n",
      "\n",
      "        {loop}\n",
      "        __syncthreads();\n",
      "    }}\n",
      "\n",
      "    if (Row < Width && Col < Width)\n",
      "    {{\n",
      "        P[Row*Width + Col] = Pvalue;\n",
      "    }}\n",
      "}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('template.cu', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template is meant to be passed to `str.format` to convert to a functioning CUDA C kernel (thus the existence of all the double curly braces, which escape to single curly braces in the `str.format` specification). It is a fairly straightforward tiled matrix multiplication kernel, the tile width `{TW}` being one of the parameters. The `{loop}` section is meant to both represent a full or unrolled version of the inner loop, which the user can also specify.\n",
    "\n",
    "The textual convertion is handled by the class `MatMulKernel` defined in `matmul.py`, which also serves to interface with the PyCUDA runtime. We can construct a kernel for double precision matmul with 16 tile width and non-unrolled loops as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matmul import MatMulKernel\n",
    "# k1 = MatMulKernel(dtype=np.float64, tile_width=16, loop_unroll=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the constructor calls the `gen_source` method to retrieve the template and perform textual transformation. The generated source code is then compiled by `nvcc` via the `compile` method and loaded onto the GPU. Alternatively, we can skip the compilation explicitly by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k1_uncompiled = MatMulKernel(dtype=np.float64, tile_width=16, loop_unroll=False, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `matmul` method of `MatMulKernel` provides a convenient interface to compute matrix multiplication using the compiled kernel. It determines the block and grid size automatically and launches CDUA kernels using PyCUDA's APIs. It also includes code to track execution time, which is used by the profiler to determine the relative performance of different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 2000\n",
    "# # Construct test matrices\n",
    "# M = np.random.randn(n, n)\n",
    "# N = np.random.randn(n, n)\n",
    "# P = M.dot(N) # sequential result\n",
    "# P_gpu = k1.matmul(M, N) # GPU result using our kernel\n",
    "# # P_gpu, milisecs = k1.matmul(M, N, timed=True)\n",
    "# err = np.max(np.abs((P - P_gpu) / P))\n",
    "# print(\"Error between CPU and GPU result: {:e}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling is done by the `tune_kernels` function defined in `autotuner.py`. `tune_kernels` accepts a matrix width `n` and data type `dtype` (can be `numpy.float32` or `numpy.float64`) and finds the best kernel for which n*n `dtype` matrix multiplication is fastest on the current CUDA device. It does this by constructing different `MatMulKernel` instances, generating two randomly constructed matrices and timing the execution of all `MatMulKernel` on these two matrices. Since GPU execution time may differ drastically between runs, a third parameter, `num_trials`, is used to determine how many trials to average. After profiling data is gathered, the parameter set with the least execution time is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autotuner import tune_kernels\n",
    "# n = 2000\n",
    "# dtype = np.float64\n",
    "# tw, unroll = tune_kernels(n, dtype) # invoke the autotuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can construct the optimal kernel using the returned parameter set to use in subsequent calculations (this is essentially what the script interface does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_optimal = MatMulKernel(dtype, tw, unroll)\n",
    "# # Do something with k_optimal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
