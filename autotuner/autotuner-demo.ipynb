{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform == Linux-3.10.0-693.11.6.el7.x86_64-x86_64-with-centos-7.4.1708-Core x86_64\n",
      "Python version == 2.7.5\n",
      "NumPy version == 1.7.1\n",
      "PyCUDA version == 2017.1.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import numpy as np\n",
    "import pycuda\n",
    "from pycuda import autoinit # set up the PyCUDA runtime\n",
    "\n",
    "print(\"Platform == \" + platform.platform() + ' ' + platform.machine())\n",
    "print(\"Python version == \" + platform.python_version())\n",
    "print(\"NumPy version == \" + np.__version__)\n",
    "print(\"PyCUDA version == \" + pycuda.VERSION_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Autotuning\n",
    "\n",
    "Autotuning is the idea of letting programs find the best parameters to an algorithm and generate the algorithm according to these parameters. It is a widely used technique in numerical computation software such as [ATLAS](http://math-atlas.sourceforge.net/) and [FFTW](http://www.fftw.org/). \n",
    "\n",
    "The use of autotuning for GPU kernels has become a hot research topic in the recent years. Two major approaches to autotuning are model-based tuning (which creates a computation model *a priori* based on the algorithm and architecture) and empirical tuning (which measures the performance as if the kernel is a black box). The latter is used for the `autotuner` package for the specific problem of GPU dense matrix multiplication.\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "Dense matrix multiplication is one of the most studied GPU applications. Although highly tuned implementations can be found with relative ease, an autotuner for matrix multiplication still has its benefits. This is because hand-tuned implementations for one device may not run as well on another device, while autotuners can be ported easily to new architectures with good performance. In addition, matrix multiplication kernels are relatively simple and serves as a good introduction to autotuner design.\n",
    "\n",
    "## PyCUDA and GPU metaprogramming\n",
    "\n",
    "[PyCUDA](https://mathema.tician.de/software/pycuda/) is a Python library for accessing Nvidiaâ€™s CUDA parallel computation API. One of the main strengths of PyCUDA is its ability to perform **just-in-time compilation (JIT)** of CUDA C source code. A programmer can create some kernels, compile and run them on the GPU, and then modify the existing kernels dynamically based on the results without leaving the Python interpreter. This process, when done in a program-controlled manner, enables **GPU metaprogramming**, which is crucial to the development of autotuners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `autotuner.py`\n",
    "\n",
    "When invoked as a Python script on the command line, `autotuner.py` parses the arguments and initiates a profiling session to find the optimal parameters for the matmul kernel (this is the autotuning part). After profiling is done, the source code for the optimal kernel is written out as a `.cu` file which can later be used either with PyCUDA or just as input to `nvcc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: autotuner.py [-h] [-t NUM_TRIALS] [-d CUDA_DEVICE] [-o OUTPUT_FILE]\n",
      "                    n [{single,double}]\n",
      "\n",
      "An autotuner that generates the best CUDA matrix multiplication kernel.\n",
      "\n",
      "positional arguments:\n",
      "  n                     matrix width\n",
      "  {single,double}       precision (single or double, default to double)\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -t NUM_TRIALS, --num-trials NUM_TRIALS\n",
      "                        number of trials (default: 5)\n",
      "  -d CUDA_DEVICE, --CUDA-device CUDA_DEVICE\n",
      "                        which CUDA device to tune on (default: 0)\n",
      "  -o OUTPUT_FILE, --output-file OUTPUT_FILE\n",
      "                        name of the output file (default: matmul.cu)\n"
     ]
    }
   ],
   "source": [
    "run autotuner.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample session using `autotuner.py` looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning matrix multiplication kernel for CUDA device GeForce GTX TITAN X\n",
      "Matrix dimension: 2000 x 2000\n",
      "Precision: single\n",
      "--------------------\n",
      "tile width = 8, unrolled loops: average run time = 48.7255935669 ms\n",
      "tile width = 8, full loops: average run time = 50.4691520691 ms\n",
      "tile width = 16, unrolled loops: average run time = 26.9700546265 ms\n",
      "tile width = 16, full loops: average run time = 26.4670078278 ms\n",
      "tile width = 24, unrolled loops: average run time = 27.0079547882 ms\n",
      "tile width = 24, full loops: average run time = 29.1127815247 ms\n",
      "tile width = 32, unrolled loops: average run time = 26.9577411652 ms\n",
      "tile width = 32, full loops: average run time = 26.9245376587 ms\n",
      "--------------------\n",
      "Best kernel: tile width = 16, loop unroll = False\n"
     ]
    }
   ],
   "source": [
    "run autotuner.py 2000 single -o kernel.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Generated by autotuner.py */\n",
      "/* tile width = 16, loop unroll = False */\n",
      "__global__ void matmul(float *M, float *N, float *P, int Width)\n",
      "{\n",
      "    // Compute M * N and store result in P\n",
      "    // M and N are Width * Width matrices\n",
      "    __shared__ float Ms[16][16];\n",
      "    __shared__ float Ns[16][16];\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "    int Row = blockIdx.y * 16 + ty;\n",
      "    int Col = blockIdx.x * 16 + tx;\n",
      "\n",
      "    float Pvalue = 0.0f;\n",
      "    for (int ph = 0; ph < ceil(Width / (float)16); ++ph)\n",
      "    {\n",
      "        // Cooperatively load tile into shared memory\n",
      "        if (Row < Width && ph*16 + tx < Width)\n",
      "        {\n",
      "            Ms[ty][tx] = M[Row*Width + ph*16 + tx];\n",
      "        }\n",
      "        else\n",
      "        {\n",
      "            Ms[ty][tx] = 0.0f;\n",
      "        }\n",
      "        if (Col < Width && ph*16 + ty < Width)\n",
      "        {\n",
      "            Ns[ty][tx] = N[(ph*16 + ty)*Width + Col];\n",
      "        }\n",
      "        else\n",
      "        {\n",
      "            Ns[ty][tx] = 0.0f;\n",
      "        }\n",
      "        __syncthreads();\n",
      "\n",
      "        for (int k = 0; k < 16; ++k)\n",
      "        {\n",
      "            Pvalue += Ms[ty][k] * Ns[k][tx];\n",
      "        }\n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    if (Row < Width && Col < Width)\n",
      "    {\n",
      "        P[Row*Width + Col] = Pvalue;\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('kernel.cu', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A closer look at the internals\n",
    "\n",
    "The base building block of each kernel is the template file `template.cu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void matmul({real} *M, {real} *N, {real} *P, int Width)\n",
      "{{\n",
      "    // Compute M * N and store result in P\n",
      "    // M and N are Width * Width matrices\n",
      "    __shared__ {real} Ms[{TW}][{TW}];\n",
      "    __shared__ {real} Ns[{TW}][{TW}];\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "    int Row = blockIdx.y * {TW} + ty;\n",
      "    int Col = blockIdx.x * {TW} + tx;\n",
      "\n",
      "    {real} Pvalue = {fzero};\n",
      "    for (int ph = 0; ph < ceil(Width / ({real}){TW}); ++ph)\n",
      "    {{\n",
      "        // Cooperatively load tile into shared memory\n",
      "        if (Row < Width && ph*{TW} + tx < Width)\n",
      "        {{\n",
      "            Ms[ty][tx] = M[Row*Width + ph*{TW} + tx];\n",
      "        }}\n",
      "        else\n",
      "        {{\n",
      "            Ms[ty][tx] = {fzero};\n",
      "        }}\n",
      "        if (Col < Width && ph*{TW} + ty < Width)\n",
      "        {{\n",
      "            Ns[ty][tx] = N[(ph*{TW} + ty)*Width + Col];\n",
      "        }}\n",
      "        else\n",
      "        {{\n",
      "            Ns[ty][tx] = {fzero};\n",
      "        }}\n",
      "        __syncthreads();\n",
      "\n",
      "        {loop}\n",
      "        __syncthreads();\n",
      "    }}\n",
      "\n",
      "    if (Row < Width && Col < Width)\n",
      "    {{\n",
      "        P[Row*Width + Col] = Pvalue;\n",
      "    }}\n",
      "}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('template.cu', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template is meant to be passed to `str.format` to convert to a functioning CUDA C kernel (thus the existence of all the double curly braces, which escape to single curly braces in the `str.format` specification). It is a fairly straightforward tiled matrix multiplication kernel, the tile width `{TW}` being one of the parameters. The `{loop}` section is meant to both represent a full or unrolled version of the inner loop, which the user can also specify.\n",
    "\n",
    "The textual convertion is handled by the class `MatMulKernel` defined in `matmul.py`, which also serves to interface with the PyCUDA runtime. We can construct a kernel for double precision matmul with 16 tile width and non-unrolled loops as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matmul import MatMulKernel\n",
    "k1 = MatMulKernel(dtype=np.float64, tile_width=16, loop_unroll=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the constructor calls the `gen_source` method to retrieve the template and perform textual transformation. The generated source code is then compiled by `nvcc` via the `compile` method and loaded onto the GPU. Alternatively, we can skip the compilation explicitly by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1_uncompiled = MatMulKernel(dtype=np.float64, tile_width=16, loop_unroll=False, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `matmul` method of `MatMulKernel` provides a convenient interface to compute matrix multiplication using the compiled kernel. It determines the block and grid size automatically and launches CDUA kernels using PyCUDA's APIs. It also includes code to track execution time, which is used by the profiler to determine the relative performance of different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error between CPU and GPU result: 8.627991e-10\n"
     ]
    }
   ],
   "source": [
    "n = 2000\n",
    "# Construct test matrices\n",
    "M = np.random.randn(n, n)\n",
    "N = np.random.randn(n, n)\n",
    "P = M.dot(N) # sequential result\n",
    "P_gpu = k1.matmul(M, N) # GPU result using our kernel\n",
    "# P_gpu, milisecs = k1.matmul(M, N, timed=True)\n",
    "err = np.max(np.abs((P - P_gpu) / P))\n",
    "print(\"Error between CPU and GPU result: {:e}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling is done by the `tune_kernels` function defined in `autotuner.py`. `tune_kernels` accepts a matrix width `n` and data type `dtype` (can be `numpy.float32` or `numpy.float64`) and finds the best kernel for which n*n `dtype` matrix multiplication is fastest on the current CUDA device. It does this by constructing different `MatMulKernel` instances, generating two randomly constructed matrices and timing the execution of all `MatMulKernel` on these two matrices. Since GPU execution time may differ drastically between runs, a third parameter, `num_trials`, is used to determine how many trials to average. After profiling data is gathered, the parameter set with the least execution time is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning matrix multiplication kernel for CUDA device GeForce GTX TITAN X\n",
      "Matrix dimension: 2000 x 2000\n",
      "Precision: double\n",
      "--------------------\n",
      "tile width = 8, unrolled loops: average run time = 155.70718689 ms\n",
      "tile width = 8, full loops: average run time = 157.938021851 ms\n",
      "tile width = 16, unrolled loops: average run time = 150.12411499 ms\n",
      "tile width = 16, full loops: average run time = 150.728338623 ms\n",
      "tile width = 24, unrolled loops: average run time = 152.241741943 ms\n",
      "tile width = 24, full loops: average run time = 152.398934937 ms\n",
      "tile width = 32, unrolled loops: average run time = 139.101806641 ms\n",
      "tile width = 32, full loops: average run time = 139.253781128 ms\n",
      "--------------------\n",
      "Best kernel: tile width = 32, loop unroll = True\n"
     ]
    }
   ],
   "source": [
    "from autotuner import tune_kernels\n",
    "n = 2000\n",
    "dtype = np.float64\n",
    "tw, unroll = tune_kernels(n, dtype) # invoke the autotuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can construct the optimal kernel using the returned parameter set to use in subsequent calculations (this is essentially what the script interface does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__global__ void matmul(double *M, double *N, double *P, int Width)\n",
      "{\n",
      "    // Compute M * N and store result in P\n",
      "    // M and N are Width * Width matrices\n",
      "    __shared__ double Ms[32][32];\n",
      "    __shared__ double Ns[32][32];\n",
      "    int tx = threadIdx.x;\n",
      "    int ty = threadIdx.y;\n",
      "    int Row = blockIdx.y * 32 + ty;\n",
      "    int Col = blockIdx.x * 32 + tx;\n",
      "\n",
      "    double Pvalue = 0.0;\n",
      "    for (int ph = 0; ph < ceil(Width / (double)32); ++ph)\n",
      "    {\n",
      "        // Cooperatively load tile into shared memory\n",
      "        if (Row < Width && ph*32 + tx < Width)\n",
      "        {\n",
      "            Ms[ty][tx] = M[Row*Width + ph*32 + tx];\n",
      "        }\n",
      "        else\n",
      "        {\n",
      "            Ms[ty][tx] = 0.0;\n",
      "        }\n",
      "        if (Col < Width && ph*32 + ty < Width)\n",
      "        {\n",
      "            Ns[ty][tx] = N[(ph*32 + ty)*Width + Col];\n",
      "        }\n",
      "        else\n",
      "        {\n",
      "            Ns[ty][tx] = 0.0;\n",
      "        }\n",
      "        __syncthreads();\n",
      "\n",
      "        Pvalue += Ms[ty][0] * Ns[0][tx];\n",
      "        Pvalue += Ms[ty][1] * Ns[1][tx];\n",
      "        Pvalue += Ms[ty][2] * Ns[2][tx];\n",
      "        Pvalue += Ms[ty][3] * Ns[3][tx];\n",
      "        Pvalue += Ms[ty][4] * Ns[4][tx];\n",
      "        Pvalue += Ms[ty][5] * Ns[5][tx];\n",
      "        Pvalue += Ms[ty][6] * Ns[6][tx];\n",
      "        Pvalue += Ms[ty][7] * Ns[7][tx];\n",
      "        Pvalue += Ms[ty][8] * Ns[8][tx];\n",
      "        Pvalue += Ms[ty][9] * Ns[9][tx];\n",
      "        Pvalue += Ms[ty][10] * Ns[10][tx];\n",
      "        Pvalue += Ms[ty][11] * Ns[11][tx];\n",
      "        Pvalue += Ms[ty][12] * Ns[12][tx];\n",
      "        Pvalue += Ms[ty][13] * Ns[13][tx];\n",
      "        Pvalue += Ms[ty][14] * Ns[14][tx];\n",
      "        Pvalue += Ms[ty][15] * Ns[15][tx];\n",
      "        Pvalue += Ms[ty][16] * Ns[16][tx];\n",
      "        Pvalue += Ms[ty][17] * Ns[17][tx];\n",
      "        Pvalue += Ms[ty][18] * Ns[18][tx];\n",
      "        Pvalue += Ms[ty][19] * Ns[19][tx];\n",
      "        Pvalue += Ms[ty][20] * Ns[20][tx];\n",
      "        Pvalue += Ms[ty][21] * Ns[21][tx];\n",
      "        Pvalue += Ms[ty][22] * Ns[22][tx];\n",
      "        Pvalue += Ms[ty][23] * Ns[23][tx];\n",
      "        Pvalue += Ms[ty][24] * Ns[24][tx];\n",
      "        Pvalue += Ms[ty][25] * Ns[25][tx];\n",
      "        Pvalue += Ms[ty][26] * Ns[26][tx];\n",
      "        Pvalue += Ms[ty][27] * Ns[27][tx];\n",
      "        Pvalue += Ms[ty][28] * Ns[28][tx];\n",
      "        Pvalue += Ms[ty][29] * Ns[29][tx];\n",
      "        Pvalue += Ms[ty][30] * Ns[30][tx];\n",
      "        Pvalue += Ms[ty][31] * Ns[31][tx];\n",
      "        \n",
      "        __syncthreads();\n",
      "    }\n",
      "\n",
      "    if (Row < Width && Col < Width)\n",
      "    {\n",
      "        P[Row*Width + Col] = Pvalue;\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_optimal = MatMulKernel(dtype, tw, unroll)\n",
    "print(k_optimal.src)\n",
    "# Do something with k_optimal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
